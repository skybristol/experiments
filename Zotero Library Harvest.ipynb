{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0e15d4-a25b-4874-8431-02bebf71b185",
   "metadata": {},
   "source": [
    "This notebook works through a process for interfacing with a Zotero Group Library to harvest or pull its contents for some particular use. This is the start to a Python package that will be leveraged to handle several types of actions we need to perform on Zotero Group Libraries that we will be registering as GeoArchive collections:\n",
    "\n",
    "* Syncing metadata and file content to deep storage on organizational infrastructue to ensure longevity\n",
    "* Registering items via a handle mechanism to provide evergreen URLs for use in citations\n",
    "* Assessing and reporting on the completeness and quality of metadata\n",
    "* Analyzing item tags against controlled vocabulary sources to establish linked data\n",
    "* Piping library contents to further NLP and other AI processing in the xDD Digital Library\n",
    "\n",
    "Our initial use case with the library of [NI 43-101 Reports](https://www.zotero.org/groups/4530692/usgs_ni_43-101_reports/library) provides a reasonably large set of documents to work out our processes. The collection is large enough that live API calls when we try to get all 15K+ documents or 6K+ tags are pretty slow, meaning that we need a process that will leverage Zotero's versioning mechanism and the \"since\" query parameter we can use with pyzotero (a Python abstraction on the Zotero REST API).\n",
    "\n",
    "To deal with this dynamic and provide a reasonably performant system, I'm breaking the architecture into two logical parts:\n",
    "\n",
    "1) A process that can be containerized to periodically check the library for changes and cache metadata files in a special [inventory item](https://www.zotero.org/groups/4530692/usgs_ni_43-101_reports/items/WSFB4RQE/library) within the library itself. The three files stored here (items, collections, and tags) can then be accessed with a read-only API connection from any downstream users. Accessing these files only takes a few seconds vs. 20+ minutes to pull everything fresh from the API.\n",
    "2) Processes that work with the file caches (which store the raw information from the API) in various ways to accomplish the value-added GeoArchive services we are layering onto Zotero libraries.\n",
    "\n",
    "Any given system could decide to bypass the file caches and work directly against the live API, guaranteeing the freshest information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2f0ce3-b169-422d-a986-ff110df265a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyzotero import zotero\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815d1ce-c92e-4d80-8f73-e4bba7b2739b",
   "metadata": {},
   "source": [
    "The Zotero API provides access to personal (user) and group libraries, with slightly different methods for each. Here we're only covering access to group libraries as this fits our use case. To instantiate an API connection with pyzotero, you need to provide the group library identifier and an API key. Library IDs are open and available to anyone. Here, we are interfacing with a library/collection of [NI 43-101 Reports](https://www.zotero.org/groups/4530692/usgs_ni_43-101_reports/library). You can see the library ID there in the URL: 4530692.\n",
    "\n",
    "An API key has to be set up by an authorized user and will specify the type of access allowed and what library or libraries the key will access. In this case, we set up a read-only API key for the specific library, which should be the best practice for this kind of system.\n",
    "\n",
    "In the following codeblock, we ask for these two parameters, hiding the API key as a secret. (Note: Contact the notebook author if you'd like to participate or set this up to work with your own specified library and API key.) In a production implementation of this idea, the Library ID and API Key parameters could be set up as environment variables for an application or some kind (e.g., Lambda process, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b045de-000d-4938-bc01-964fcb99cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "API Key ························\n"
     ]
    }
   ],
   "source": [
    "zotero_api_key = getpass(\"API Key\")\n",
    "\n",
    "inventory_item_key = \"WSFB4RQE\"\n",
    "zotero_library_id = \"4530692\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d2882-7107-4623-882e-a5c7369245b4",
   "metadata": {},
   "source": [
    "# Common Functions\n",
    "These are functions that will be in common between read-only and write operations with Zotero. They include making a basic connection to a group library with a library ID and an API key, getting the inventory item and file attachments, loading the raw inventory metadata (items, collections, and tags), and putting the essential inventory data into Pandas dataframes. The latter includes some specialized logic for parsing what are essentially simple compound identifiers for tags that help us align them with vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78a49b-0e67-47ba-b9e4-12df16dd016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zotero_library(library_id, api_key):\n",
    "    # Establish connection to library with an ID and API Key\n",
    "    zot = zotero.Zotero(\n",
    "        library_id=library_id,\n",
    "        library_type='group',\n",
    "        api_key=api_key,\n",
    "    )\n",
    "    return zot\n",
    "\n",
    "def get_inventory_files(z, inventory_item):\n",
    "    inventory_files = z.children(inventory_item, itemType=\"attachment\")\n",
    "    inventory_file_items = {\n",
    "        \"inventory_item\": z.item(inventory_item),\n",
    "        \"inventory_item_keys\": [inventory_item] + [i[\"key\"] for i in inventory_files]\n",
    "    }\n",
    "    for file_type in [\"items\",\"collections\",\"tags\"]:\n",
    "        inventory_file_items[file_type] = next(\n",
    "            (\n",
    "                i for i in inventory_files \n",
    "                if i[\"data\"][\"filename\"] == f\"{file_type}.json\"\n",
    "            ), None)\n",
    "        \n",
    "    return inventory_file_items\n",
    "\n",
    "def load_raw_inventory(z, inventory_item):\n",
    "    existing_files = get_inventory_files(z, inventory_item)\n",
    "\n",
    "    inventory = {\n",
    "        \"inventory_item_keys\": existing_files[\"inventory_item_keys\"],\n",
    "        \"inventory_files\": existing_files\n",
    "    }\n",
    "    for file_type in [\"items\",\"collections\",\"tags\"]:\n",
    "        raw_data = z.file(existing_files[file_type][\"key\"])\n",
    "        inventory[file_type] = raw_data\n",
    "    \n",
    "    return inventory\n",
    "\n",
    "def load_df_inventory(raw_inventory):\n",
    "    inventory = {}\n",
    "    for k,v in raw_inventory.items():\n",
    "        if k == \"tags\":\n",
    "            inventory[\"tags\"] = pd.DataFrame([\n",
    "                {\n",
    "                    \"tag\": i,\n",
    "                    \"type\": i.split(\":\")[0],\n",
    "                    \"value\": i.split(\":\")[-1]\n",
    "                } for i in v if \":\" in i\n",
    "            ])\n",
    "        elif k in [\"items\",\"collections\"]:\n",
    "            inventory[k] = pd.DataFrame([i[\"data\"] for i in v])\n",
    "\n",
    "    return inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61f0a6b3-b8bc-409f-ae31-8449e7bb7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "zot = zotero_library(zotero_library_id, zotero_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689218e2-0591-4539-abf6-c269217e4a85",
   "metadata": {},
   "source": [
    "# Baselining a Library\n",
    "The following functions handle the process of initially baselining items, collections, and tags for a given library into local cache files in JSON format and then uploading those to a specified inventory item in the Zotero library. This could be a little more robust to do things like discover the inventory item based on characteristics or create an inventory item that doesn't already exist, but I chose to make this very explicit by setting the actual key value for the item we want to use in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d1c3abe-c742-4aed-912b-6cc1d68c049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_tags(z, output_path=\"data\"):\n",
    "    tags = z.everything(z.tags())\n",
    "    json.dump(tags, open(f\"{output_path}/tags.json\", \"w\"))\n",
    "\n",
    "def baseline_collections(z, output_path=\"data\"):\n",
    "    records = z.everything(z.collections())\n",
    "    json.dump(records, open(f\"{output_path}/collections.json\", \"w\"))\n",
    "\n",
    "def baseline_items(z, inventory_item, output_path=\"data\"):\n",
    "    records = z.everything(z.items())\n",
    "    \n",
    "    inventory_files = get_inventory_files(z, inventory_item)\n",
    "    records = [\n",
    "        i for i in records \n",
    "        if i[\"key\"] not in inventory_files[\"inventory_item_keys\"]\n",
    "    ]\n",
    "    json.dump(records, open(f\"{output_path}/items.json\", \"w\"))\n",
    "\n",
    "def baseline_cache(\n",
    "    z, \n",
    "    inventory_item, \n",
    "    output_path=\"data\"\n",
    "):\n",
    "    inventory_files = get_inventory_files(z, inventory_item)\n",
    "\n",
    "    new_uploads = []\n",
    "    for file_type in [\"items\",\"collections\",\"tags\"]:\n",
    "        if os.path.exists(f\"{output_path}/{file_type}.json\"):\n",
    "            if inventory_files[file_type] is not None:\n",
    "                z.delete_item(z.item(inventory_files[file_type][\"key\"]))\n",
    "                print(\"Deleted existing cache file for\", file_type)\n",
    "            new_uploads.append(os.path.abspath(f\"{output_path}/{file_type}.json\"))\n",
    "        \n",
    "    if new_uploads:\n",
    "        inventory_item_update = {\n",
    "            \"key\": inventory_item,\n",
    "            \"version\": inventory_files[\"inventory_item\"][\"data\"][\"version\"],\n",
    "            \"extra\": z.last_modified_version()\n",
    "        }\n",
    "        z.update_item(inventory_item_update)\n",
    "        print(\"Updated inventory item with last modified version for the library\")\n",
    "        \n",
    "        z.attachment_simple(new_uploads, parentid=inventory_item)\n",
    "        print(\"Created new files in inventory\", new_uploads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136e7039-0c28-4378-86b7-0c9d0d975469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 912 ms, sys: 32.2 ms, total: 944 ms\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_tags(zot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f2ba8d3-2548-4bef-b0f4-4ac07f7a9a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 ms, sys: 3.87 ms, total: 37.5 ms\n",
      "Wall time: 2.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "baseline_collections(zot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "448a5154-6666-47c5-b45c-c60aad13ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.55 s, sys: 314 ms, total: 8.86 s\n",
      "Wall time: 24min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_items(zot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f272cb0-919f-4e4b-a543-8641d76e5d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing cache file for items\n",
      "Deleted existing cache file for collections\n",
      "Deleted existing cache file for tags\n",
      "Updated inventory item with last modified version for the library\n",
      "Created new files in inventory ['/home/jovyan/experiments/data/items.json', '/home/jovyan/experiments/data/collections.json', '/home/jovyan/experiments/data/tags.json']\n",
      "CPU times: user 287 ms, sys: 20.3 ms, total: 307 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_cache(zot, inventory_item_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c919e16-2ec3-4579-9114-edccfd1728bc",
   "metadata": {},
   "source": [
    "# Updating the Inventory\n",
    "Updating the inventory involves several steps that need to operate in series. These could be made more robust in the code by building a Python class.\n",
    "\n",
    "1) Load the raw inventory files from the specified Zotero inventory item\n",
    "2) Determine the applicable sequenced version numbers present in the cached inventories\n",
    "3) Check for any deletions that have happened since the last cache and remove those from the inventories (a given system may need to handle deletions in a particular way)\n",
    "4) Get new items, collections, and tags using the version numbers determined from the previous cache\n",
    "5) Remove the updated records from the previous cache and add new/updated records to the new caches\n",
    "6) Save the new cache files to some mounted disc\n",
    "7) Push the new cache files to the Zotero inventory item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8817871d-63b7-460e-8cff-559b11b32563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inventory(z, inventory_item, output_path=\"data\"):\n",
    "    inventory_files = get_inventory_files(z, inventory_item)\n",
    "    raw_inventory = load_raw_inventory(zot, inventory_item_key)\n",
    "\n",
    "    cache_item_version = max([i[\"version\"] for i in raw_inventory[\"items\"]])\n",
    "    cache_collection_version = max([i[\"version\"] for i in raw_inventory[\"collections\"]])\n",
    "    min_cache_version = min([cache_item_version, cache_collection_version])\n",
    "\n",
    "    deletions = z.deleted(since=min_cache_version)\n",
    "    \n",
    "    new_inventory = {}\n",
    "    for x in [\"items\",\"collections\"]:\n",
    "        new_inventory[x] = [\n",
    "            i for i in raw_inventory[x] if i[\"key\"] not in deletions[x]\n",
    "        ]\n",
    "    new_inventory[\"tags\"] = [\n",
    "        i for i in raw_inventory[\"tags\"] if i not in deletions[\"tags\"]\n",
    "    ]\n",
    "    \n",
    "    new_items = z.everything(z.items(since=cache_item_version))\n",
    "    new_collections = z.everything(z.collections(since=cache_collection_version))\n",
    "    new_tags = z.everything(z.tags(since=cache_item_version))\n",
    "    \n",
    "    if new_tags:\n",
    "        new_inventory[\"tags\"].extend(new_tags)\n",
    "        new_inventory[\"tags\"] = list(set(new_inventory[\"tags\"]))\n",
    "        \n",
    "    if new_collections:\n",
    "        new_inventory[\"collections\"] = [\n",
    "            i for i in new_inventory[\"collections\"]\n",
    "            if i[\"key\"] not in [\n",
    "                x[\"key\"] for x in new_collections\n",
    "            ]\n",
    "        ]\n",
    "        new_inventory[\"collections\"].extend(new_collections)\n",
    "        \n",
    "    if new_items:\n",
    "        items_wo_inventory = [\n",
    "            i for i in new_items \n",
    "            if i[\"key\"] not in raw_inventory[\"inventory_item_keys\"]\n",
    "        ]\n",
    "        if items_wo_inventory:\n",
    "            new_inventory[\"items\"] = [\n",
    "                i for i in new_inventory[\"items\"]\n",
    "                if i[\"key\"] not in [\n",
    "                    x[\"key\"] for x in new_items\n",
    "                ]\n",
    "            ]\n",
    "            new_inventory[\"items\"].extend(items_wo_inventory)\n",
    "    \n",
    "    new_uploads = []\n",
    "    for x in [\"items\",\"collections\",\"tags\"]:\n",
    "        if raw_inventory[x] != new_inventory[x]:\n",
    "            fp = f\"{output_path}/{x}.json\"\n",
    "            json.dump(new_inventory[x], open(fp, \"w\"))\n",
    "            print(f\"WROTE {len(new_inventory[x])} RECORDS TO {fp}\")\n",
    "            \n",
    "            cache_file_key = raw_inventory[\"inventory_files\"][x][\"key\"]\n",
    "            z.delete_item(z.item(cache_file_key))\n",
    "            print(f\"DELETED PREVIOUS CACHE FILE FOR {x}: {cache_file_key}\")\n",
    "            new_uploads.append(os.path.abspath(fp))\n",
    "\n",
    "    if new_uploads:\n",
    "        inventory_item_update = {\n",
    "            \"key\": inventory_item,\n",
    "            \"version\": inventory_files[\"inventory_item\"][\"data\"][\"version\"],\n",
    "            \"extra\": z.last_modified_version()\n",
    "        }\n",
    "        z.update_item(inventory_item_update)\n",
    "        print(\"Updated inventory item with last modified version for the library\")\n",
    "        \n",
    "        z.attachment_simple(new_uploads, parentid=inventory_item)\n",
    "        print(\"Created new files in inventory\", new_uploads)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aae6e3e2-d5ff-422b-ab35-ce1c8cf0a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE 30993 RECORDS TO data/items.json\n",
      "DELETED PREVIOUS CACHE FILE FOR items: SWD5EPAI\n",
      "Updated inventory item with last modified version for the library\n",
      "Created new files in inventory ['/home/jovyan/experiments/data/items.json']\n",
      "CPU times: user 4.33 s, sys: 340 ms, total: 4.66 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "update_inventory(zot, inventory_item_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe87a20-b30e-46b9-bd1b-7da68694f098",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Caching Library Metadata\n",
    "For a Zotero Group Library that will be designated as an official GeoArchive repository, we will end up having a variety of services that periodically read information from the library and add services of perform functions.\n",
    "* Backup metadata and file content to help ensure long-term viability\n",
    "* Register identifiers/links to items via some form of \"evergreen URL\" capability\n",
    "* Send items and content to additional value-added services (e.g., xDD AI engines)\n",
    "\n",
    "To do this work, we will need to interact with collection and item metadata and file attachment content in various ways. After experimenting with various ways of working with the Zotero API and architecting a working system, it ended up making some sense to break up processing such that we periodically read from the library and cache some working files back to the library itself. A full baseline library item retrieval is pretty costly and time consuming via the Zotero API, even if we break up requests and operate them in parallel. The versioning mechanism and supported \"since\" queries provide a great way to periodically go after new and updated information once we have a baseline set.\n",
    "\n",
    "I'm experimenting with setting up a top-level collection called \"inventory\" that will contain three JSON file attachments with all the essential data we need to operate against (collections, items, and tags). We can create a container app that will use a read/write API key to refresh these files on some scheduled basis. Downstream apps can then avoid needing to run their own costly API calls but instead check the files for updates compared against their own logs or other ways of checking for new information and then retrieve and operate using the file contents. Reading in a stored JSON file is immensely faster and less costly than running an API call for the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5378cff2-b4a3-438f-8a50-7a8002b77df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inventory = load_raw_inventory(zot, inventory_item_key)\n",
    "df_inventory = load_df_inventory(raw_inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398ed409-8fd0-46d6-8caa-6d7abaf5ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_inventory = update_stashes(zot, inventory_item_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e797b4-c3d4-436e-904c-4f2d9326575c",
   "metadata": {},
   "source": [
    "# Collection-based Metadata\n",
    "The organization scheme used in a Zotero Group Library could provide different types of value-added metadata about items. Collections are used like folders to provide a visual organizational scheme to library items, helping library users navigate through and work within a given library, particularly one that contains a lot of items.\n",
    "\n",
    "In the case of the NI 43-101 Reports here, we replicated a scheme that has been used for years where these reports have been accumulating on network shared folders. It uses a hierarchy of folder names to indicate a region of the world and country as well as US States for mining projects that are the subject of the reports. Report items can be found at any of these levels with the immediate parent collection indicating the most specific geographic context available in the hierarchical structure. We may add a further level of structure with project names in future.\n",
    "\n",
    "Since this information is contained in the organizational structure of the items and does not necessarily exist in individual item metadata (e.g., tags), we need to incorporate an \"interpretation\" of the organizational structure into our process of assembling useful metadata. In this case, the geographic context for a given report item may be useful in driving NLP processes. It can narrow the field on other, more specific place names we might work to recognize in full text or serve as a validation element in extracting point coordinates or other geospatial information.\n",
    "\n",
    "Each process like this for reading a Zotero library and digesting its metadata is going to be different. The collection organization scheme is going to be different, introducing different types of information to the process. However, something like what we present here can be extended to essentially classify the collections in a given library as metadata elements of different kinds. For this case, we pull all collections and then use the hierarchical structure (via \"parentCollection\") to assign a \"type\" value of Region, Country, or US State to each collection. This essentially gives us an additional set of geographic context tags we can send along with our items for further use downstream.\n",
    "\n",
    "When retrieving items (see below), we will get a list of collection keys that an item belongs to. We can also request items for a given collection. Any library item can belong to more than one collection. This essentially means we can use our collection metadata we assemble below to \"backfill\" additional value-added item metadata based on whatever significance the collections provide.\n",
    "\n",
    "Organizing a relatively large library like the NI 43-101 Reports case into collections does present an opportunity to break up and parallelize processing of the library in various ways. The collection_items() function in pyzotero provides a way of retrieving the items of a given collection without delving into its subcollections, allowing us to work through a set of collections in a library, retrieving items and processing them in batches. \n",
    "\n",
    "Note: We did do some normalization work in our process of building out the Zotero library structure for the NI 43-101 Reports to ensure that each collection name can be resolved to official ISO sources for countries and US States. Regions represent continents and a few sub-continent but recognizable region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716bc34f-4f62-426a-b02a-079269e1fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def library_collections(z):\n",
    "    all_collections = z.everything(z.collections())\n",
    "\n",
    "    collections = [\n",
    "        {\n",
    "            \"type\": \"Region\", \n",
    "            \"key\": i[\"key\"], \n",
    "            \"name\": i[\"data\"][\"name\"]\n",
    "        } for i in all_collections if not i[\"data\"][\"parentCollection\"]\n",
    "    ]\n",
    "\n",
    "    for r in collections:\n",
    "        collections.extend([\n",
    "            {\n",
    "                \"type\": \"Country\", \n",
    "                \"key\": i[\"key\"], \n",
    "                \"name\": i[\"data\"][\"name\"],\n",
    "                \"parent\": r[\"key\"]\n",
    "            } for i in all_collections if i[\"data\"][\"parentCollection\"] == r[\"key\"]\n",
    "        ])\n",
    "\n",
    "    us_collection = next((i for i in collections if i[\"name\"] == \"United States\"), None)\n",
    "\n",
    "    collections.extend([\n",
    "        {\n",
    "            \"type\": \"US State\", \n",
    "            \"key\": i[\"key\"], \n",
    "            \"name\": i[\"data\"][\"name\"],\n",
    "            \"parent\": us_collection[\"key\"]\n",
    "        } for i in all_collections if i[\"data\"][\"parentCollection\"] == us_collection[\"key\"]\n",
    "    ])\n",
    "    \n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f100b6ec-e6e7-4d03-bd51-6e78503f8ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>key</th>\n",
       "      <th>name</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Region</td>\n",
       "      <td>KVSV3898</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Region</td>\n",
       "      <td>ZNCQ3V3A</td>\n",
       "      <td>Middle East</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Region</td>\n",
       "      <td>BTJTAMND</td>\n",
       "      <td>Central America</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Region</td>\n",
       "      <td>MQWZWWM2</td>\n",
       "      <td>North America</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Region</td>\n",
       "      <td>BTZ7EAKX</td>\n",
       "      <td>Caribbean</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>US State</td>\n",
       "      <td>VGKU7ZGD</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>AKV3WJ97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>US State</td>\n",
       "      <td>5CQMNT5V</td>\n",
       "      <td>Florida</td>\n",
       "      <td>AKV3WJ97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>US State</td>\n",
       "      <td>W8R6UH2U</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AKV3WJ97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>US State</td>\n",
       "      <td>JK8JU3E2</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>AKV3WJ97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>US State</td>\n",
       "      <td>8JC7F475</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>AKV3WJ97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         type       key             name    parent\n",
       "0      Region  KVSV3898           Europe       NaN\n",
       "1      Region  ZNCQ3V3A      Middle East       NaN\n",
       "2      Region  BTJTAMND  Central America       NaN\n",
       "3      Region  MQWZWWM2    North America       NaN\n",
       "4      Region  BTZ7EAKX        Caribbean       NaN\n",
       "..        ...       ...              ...       ...\n",
       "163  US State  VGKU7ZGD          Georgia  AKV3WJ97\n",
       "164  US State  5CQMNT5V          Florida  AKV3WJ97\n",
       "165  US State  W8R6UH2U          Arizona  AKV3WJ97\n",
       "166  US State  JK8JU3E2           Alaska  AKV3WJ97\n",
       "167  US State  8JC7F475          Alabama  AKV3WJ97\n",
       "\n",
       "[168 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections = library_collections(zot)\n",
    "pd.DataFrame(collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8341115-e3f4-4cab-9272-e59c462dad3f",
   "metadata": {},
   "source": [
    "# Baselining Items\n",
    "Via the Zotero API, everything in a given library is an item. This includes metadata principals for things like reports and articles, but file attachments are items along with collections and notes and anything else that is uniquely identified and accessed. The various item methods of pyzotero provide access to items and can be used in various ways to access and work through the contents of a library.\n",
    "\n",
    "For our use case of retrieving documents for processing with the xDD tools, we will be focusing on pulling report type items for essential metadata, blending in collection item information (per the above discussion), to create xDD records. We'll also pull the file items attached to the report metadata items to get PDF document content for processing. Eventually, we will access and pull annotation items also attached to report items for value-added annotation that can be used in NLP training.\n",
    "\n",
    "Again, each case may be a little bit different in terms of how a given library is organized and managed, resulting in some tweaks to this process for other circumstances. Hopefully, the basic patterns developed here can serve as a start to a more generalized process.\n",
    "\n",
    "Baselining a given library for the purposes of creating an xDD collection or some similar purpose means working at a point in time to pull together all necessary item information translated into how the target system understands \"item\" for its context. \n",
    "\n",
    "After a library has been baselined, Zotero provides a useful versioning system that should be fairly convenient to use. Every change to any item results in a sequential version number being incremented. The version number is within the context of a given library (group or user). The last_modified_version() method of pyzotero can be used to simply retrieve the highest incremental version number across everything in the library. Usage patterns could include recording the high version number of the library at the point of sync and/or recording individual item versions and then using those within context for an API call, depending on how a workflow is composed. \n",
    "\n",
    "The API accepts a \"since\" parameter with version numbers that will only pull items changed after that version. This can be combined with other API search parameters to look for items changed within a given context (e.g., report type items changed after the last recorded version retrieved). In syncing items from a Zotero library to some other system, version numbers should be recorded and then used to go after newer items. The version number incremental system also applies for new items added to a library, so that if you ask for everything since the last version in a synced collection, you will get new items as well as updates.\n",
    "\n",
    "Deleted items are a different issue that should also be taken into account. The pyzotero API provides a deleted method with the \"since\" parameter that returns lists of collections, items, and other things that were removed from the library. How these are dealt with in a target system like xDD is another consideration we'll have to work through, but the API method provides for pulling the deletions and deciding what actions to take.\n",
    "\n",
    "The actual bibliographic metadata for items in a Zotero library is pretty straightforward and aligns well with various bibliographic metadata standards with information on title, authors, publishers, etc. The API allows for different standard forms of metadata to be requested, with the default being Zotero's own JSON format, so a harvester could be written to use something more standardized like bibtex. However, the Zotero JSON structure provides everything and is likely the easiest thing to build a comprehensize process upon.\n",
    "\n",
    "The different itemTypes in Zotero control the schema for items, and the information elements in a given itemType schema are fixed. This introduces some challenges in recording all of the information we might want to record, but it produces a predictable structure with little room for interpretation on the downstream end. Pulling items into something like the xDD Digital Library means mapping the information elements to be encountered in Zotero libraries to the target schema.\n",
    "\n",
    "In the xDD case, the target schema (viewable in the \"fields\" object [here](https://geodeepdive.org/api/articles)) is also very limited with a barebones set of identification metadata. So, our task of mapping from the barebones NI 43-101 Report metadata to the xDD \"article\" target is not difficult.\n",
    "\n",
    "What will be more interesting is working through how we introduce what will essentially be value-added annotation into this process - additional metadata about the documents in the collection that we want to leverage in NLP processing. Geographic context and important name identifiers (projects, names of active mines, mineral and other commodities, etc.) are all important organizing principles that we are capturing anyway and recording through collections and tags. We may start introducing additional attachments on report items containing explicit geospatial data or more precise temporal information. We will be encouraging scientists and support staff to annotate PDF documents and use Zotero's capabilities to extract annotation (highlights and notes) as additional attachments in Markdown format. These processes will all introduce additional content that we want xDD and other systems to access and take advantage of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c78aa1-714d-4ce7-b81f-7f53f20cc999",
   "metadata": {},
   "source": [
    "Baselining the cache of a given library is a potentially resource-intensive process, with the biggest time delay in reading what we need from the Zotero API. The API is limited to 50 items in a given request as a throttling mechanism, with pyzotero offering a generator and a couple of other methods for pulling more items. After experimenting with several ways of breaking up the requests, I found that we don't end up buying very much through various parallelization strategies and it will be more simple and reliable to use the everything() wrapper in pyzotero to grab every item we need to operate on. The benchmark I ran comparing an everything request for 30,993 metadata and attachment items in the library vs. a Dask delayed process with 4 workers resulted in 21 minutes for the former and 13 minutes for the latter but a loss of some number of records that I couldn't explain but probably had to do with some API failure I didn't catch.\n",
    "\n",
    "It does make sense, though, to get our items and then cache them as a file so that a) we can operate on that same file in different ways and b) we can store the file and use it to determine when the library contains something new for us to work on.\n",
    "\n",
    "The three types of things we need from the Zotero library are collections (worked up previously), metadata items (reports and/or other itemTypes), and attachments. The pyzotero items() function, wrapped with everything(), will get us all of our items, or if we add the \"since\" parameter with a version number, we can get anything new.\n",
    "\n",
    "Strategies for caching a library's raw information can vary depending on the platform where things are being implemented. If we're using a versioned file system like S3, we might opt to use that. Or we may come up with some file naming convention that works for our circumstance. In this rough workup, I'm simply using a local mounted file system and passing in a relative path. This will need to be tweaked to cover a broader range of use cases.\n",
    "\n",
    "I still need to work up the following:\n",
    "\n",
    "* Process for checking latest version in our cache against the last_modified_version from the library, getting new/changed items, and then either dealing with those separately or integrating them into our cache.\n",
    "* Process for dealing with deleted items.\n",
    "* Process for mapping Zotero metadata to a target schema like xDD articles.\n",
    "* Process for managing file attachment content.\n",
    "\n",
    "I split this up into three logical parts:\n",
    "1) Get items for a given collection. This is the part that needs to make an API call to Zotero and may take the most resources to operate.\n",
    "2) Process the metadata. This takes the raw item content and maps it to a profile, extracting the elements we need for that profile. It also handles blending in any additional metadata that might be useful but is outside the strict mapping to the target profile.\n",
    "3) Get file attachment keys. The Zotero API offers a couple of different ways of retrieving files. They can be streamed through as bytes to some other process or sent directly to a file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5779581-9cfd-42c4-863d-713fa4cb07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_library(z, output_path, output_file):\n",
    "    items = z.everything(z.items())\n",
    "    json.dump(items, open(f\"{output_path}/{output_file}.json\", \"w\"))\n",
    "    return len(items)\n",
    "\n",
    "def get_cached_library(library_path, library_file):\n",
    "    items = json.load(open(f\"{library_path}/{library_file}.json\", \"r\"))\n",
    "    return items\n",
    "\n",
    "def cached_last_version(items):\n",
    "    return max([i[\"version\"] for i in items])\n",
    "\n",
    "def z_collection_files(collection_items, file_type=\"application/pdf\"):\n",
    "    item_files = [\n",
    "        {\n",
    "            \"item_key\": i[\"key\"],\n",
    "            \"attachment_key\": i[\"links\"][\"attachment\"][\"href\"].split(\"/\")[-1]\n",
    "        } for i in collection_items if \"attachment\" in i[\"links\"]\n",
    "        and i[\"links\"][\"attachment\"][\"attachmentType\"] == file_type\n",
    "    ]\n",
    "    \n",
    "    return item_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc6a73f-58fa-4fbe-be5a-7f8461917689",
   "metadata": {},
   "outputs": [],
   "source": [
    "africa = next((i for i in collections if i[\"name\"] == \"Africa\"), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "440ecb7d-d2c3-460e-aba7-18a09d0d6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = json.load(open(\"data/ni43101.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83522ca3-58d0-4a2f-8a71-0ce75d91a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45844"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([i[\"version\"] for i in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d5f422a-388e-40c1-b6ef-f4e62a06e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45848"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zot.last_modified_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d630cf5-cf55-44d5-98df-e5c7fb6593b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zot.items(since=45844)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c1d0243-28cd-453f-aa62-70f2ceba14f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 s, sys: 340 ms, total: 5.56 s\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "items = zot.everything(zot.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab243148-fc19-45e1-8814-d000dd052a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'TPHFX8G9',\n",
       " 'version': 45843,\n",
       " 'library': {'type': 'group',\n",
       "  'id': 4530692,\n",
       "  'name': 'USGS NI 43-101 Reports',\n",
       "  'links': {'alternate': {'href': 'https://www.zotero.org/groups/usgs_ni_43-101_reports',\n",
       "    'type': 'text/html'}}},\n",
       " 'links': {'self': {'href': 'https://api.zotero.org/groups/4530692/items/TPHFX8G9',\n",
       "   'type': 'application/json'},\n",
       "  'alternate': {'href': 'https://www.zotero.org/groups/usgs_ni_43-101_reports/items/TPHFX8G9',\n",
       "   'type': 'text/html'},\n",
       "  'up': {'href': 'https://api.zotero.org/groups/4530692/items/HW37Q533',\n",
       "   'type': 'application/json'},\n",
       "  'enclosure': {'type': 'application/pdf',\n",
       "   'href': 'https://api.zotero.org/groups/4530692/items/TPHFX8G9/file/view',\n",
       "   'title': '4de78ee7-5acb-4f84-a6fa-b2b541e1432e.pdf',\n",
       "   'length': 38806055}},\n",
       " 'meta': {'createdByUser': {'id': 1119084,\n",
       "   'username': 'skybristol',\n",
       "   'name': 'Sky Bristol',\n",
       "   'links': {'alternate': {'href': 'https://www.zotero.org/skybristol',\n",
       "     'type': 'text/html'}}},\n",
       "  'numChildren': 0},\n",
       " 'data': {'key': 'TPHFX8G9',\n",
       "  'version': 45843,\n",
       "  'parentItem': 'HW37Q533',\n",
       "  'itemType': 'attachment',\n",
       "  'linkMode': 'imported_file',\n",
       "  'title': '4de78ee7-5acb-4f84-a6fa-b2b541e1432e.pdf',\n",
       "  'accessDate': '',\n",
       "  'url': '',\n",
       "  'note': '',\n",
       "  'contentType': 'application/pdf',\n",
       "  'charset': '',\n",
       "  'filename': '4de78ee7-5acb-4f84-a6fa-b2b541e1432e.pdf',\n",
       "  'md5': '21b6c42d5c4188a66ac051883bf0748e',\n",
       "  'mtime': 1634913913000,\n",
       "  'tags': [],\n",
       "  'relations': {},\n",
       "  'dateAdded': '2021-12-16T20:08:42Z',\n",
       "  'dateModified': '2021-12-16T20:08:42Z'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(i for i in items if i[\"data\"][\"itemType\"] == \"attachment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0aba337b-2372-4f86-8b62-4c511975bba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'QZM6W75P',\n",
       " 'version': 6682,\n",
       " 'library': {'type': 'group',\n",
       "  'id': 4530692,\n",
       "  'name': 'USGS NI 43-101 Reports',\n",
       "  'links': {'alternate': {'href': 'https://www.zotero.org/groups/usgs_ni_43-101_reports',\n",
       "    'type': 'text/html'}}},\n",
       " 'links': {'self': {'href': 'https://api.zotero.org/groups/4530692/items/QZM6W75P',\n",
       "   'type': 'application/json'},\n",
       "  'alternate': {'href': 'https://www.zotero.org/groups/usgs_ni_43-101_reports/items/QZM6W75P',\n",
       "   'type': 'text/html'},\n",
       "  'attachment': {'href': 'https://api.zotero.org/groups/4530692/items/JS2XPAMU',\n",
       "   'type': 'application/json',\n",
       "   'attachmentType': 'application/pdf',\n",
       "   'attachmentSize': 22635765}},\n",
       " 'meta': {'createdByUser': {'id': 1119084,\n",
       "   'username': 'skybristol',\n",
       "   'name': 'Sky Bristol',\n",
       "   'links': {'alternate': {'href': 'https://www.zotero.org/skybristol',\n",
       "     'type': 'text/html'}}},\n",
       "  'parsedDate': '2018',\n",
       "  'numChildren': 1},\n",
       " 'data': {'key': 'QZM6W75P',\n",
       "  'version': 6682,\n",
       "  'itemType': 'report',\n",
       "  'title': 'NI 43-101 Technical Report (PFS) for the Arrow - Rook Project in Canada dated 2018',\n",
       "  'creators': [],\n",
       "  'abstractNote': '',\n",
       "  'reportNumber': '',\n",
       "  'reportType': 'NI 43-101 Technical Report (PFS)',\n",
       "  'seriesTitle': 'Project: Arrow - Rook',\n",
       "  'place': 'Canada',\n",
       "  'institution': '',\n",
       "  'date': '2018',\n",
       "  'pages': '',\n",
       "  'language': '',\n",
       "  'shortTitle': '',\n",
       "  'url': '',\n",
       "  'accessDate': '',\n",
       "  'archive': '',\n",
       "  'archiveLocation': '',\n",
       "  'libraryCatalog': '',\n",
       "  'callNumber': '7985285f-3823-4825-b714-542e19d6ac8c',\n",
       "  'rights': '',\n",
       "  'extra': '11-2018',\n",
       "  'tags': [{'tag': 'Commodity:I'},\n",
       "   {'tag': 'Commodity:U'},\n",
       "   {'tag': 'Project:Arrow - Rook'}],\n",
       "  'collections': [],\n",
       "  'relations': {},\n",
       "  'dateAdded': '2021-12-07T19:50:04Z',\n",
       "  'dateModified': '2021-12-07T19:50:04Z'}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25749a1-9560-485d-a4cf-a7b7f5da425e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zot]",
   "language": "python",
   "name": "conda-env-zot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
