{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc62be15-7819-477a-9ea1-50fa989cdfd3",
   "metadata": {},
   "source": [
    "This notebook takes a look at the official data release product metadata flagged to the Science Centers that are most closely affiliated with the Energy and Minerals Mission Area. I was interested in exploring where we seem to have very similar or identical processing step information between data products, potentially indicating areas ripe for exploration of a new process or way of pursuing continuous data release.\n",
    "\n",
    "To do this, I focused on the data release products officially cataloged via the USGS \"public data listing\" - our Science Data Catalog (SDC). These all should have standard metadata in one of a couple possible formats, and the SDC is accessible via an API that should allow me to access metadata programmatically and work with the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27968ac5-f86c-4c4c-95d7-0ced34dabfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14812efe-a41e-4008-a802-fecb5397ba17",
   "metadata": {},
   "source": [
    "Filtering to the items of interest here is a little challenging. There are currently 5 Science Centers which receive the bulk of funding from the Mineral Resources Program and/or Energy Resources Program. This effectively puts those Centers \"in\" the Energy and Minerals Mission Area (but not exactly). Science Center names change over time, and not all USGS systems share the same \"understanding\" of what our Science Centers are. I ended up crafting a hard list of the Center names as the SDC has recorded them in its own attempt to standardize. This ends up looping in more records than actually apply, especially in the case of an \"interdisciplinary science center\" like Alaska. But it suffices to enable us to grab most of the records we want to evaluate in this process.\n",
    "\n",
    "There's also a specific API route needed to get at the original raw metadata files that are pulled into the SDC harvest process, which is where I need to go in order to get at deeper level metadata like lineage and entity/attribute information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c5fe13-75b8-437e-b8f5-4e0a61928690",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_orgs = [\n",
    "    \"Geology, Geophysics, and Geochemistry Science Center\",\n",
    "    \"Central Energy Resources Science Center\",\n",
    "    \"Geology, Mineral, Energy, and Geophysics Science Center\",\n",
    "    \"Geology, Energy, & Minerals Science Center\",\n",
    "    \"Florence Bascom Geoscience Center\",\n",
    "    \"Alaska Science Center\",\n",
    "    \"Mineral Resources Program\",\n",
    "    \"Energy Resources Program\"\n",
    "]\n",
    "\n",
    "files_api = \"https://data.usgs.gov/datacatalog/api/harvest/files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43508a-9cc7-48bd-ae28-32e419df46d3",
   "metadata": {},
   "source": [
    "I threw in a couple functions to handle the basic API queries and processing of metadata content. While this isn't a massive haul, I'm still fiddling with the best way to store and process everything, so it made sense to store a cache of raw XML content in a local directory and then some additional derivatives for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f598c59a-748d-4756-a0be-190940aad51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_org_md(org):\n",
    "    api = f\"{files_api}?data_source_name={org}&size=10000\"\n",
    "    r = requests.get(api).json()\n",
    "    return [(i[\"id\"], i[\"metadata_pid\"]) for i in r[\"items\"]]\n",
    "\n",
    "def cache_md_record(item):\n",
    "    path = f\"data/emma_md/{item['md_pid']}.xml\"\n",
    "    r = requests.get(item['md_url'])\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(r.text)\n",
    "            \n",
    "def meta_to_dict(pid):\n",
    "    path = f\"data/emma_md/{pid}.xml\"\n",
    "    with open(path) as f:\n",
    "        return xmltodict.parse(f.read(), dict_constructor=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8addf-0553-4ecd-a72b-89ef4d89122c",
   "metadata": {},
   "source": [
    "This is not a great way to go get metadata, but I've had issues in the past trying to operate in parallel or run a bunch of requests against data.usgs.gov end points because of WAF restrictions or application limitations. This block runs queries on the list of organization names and gives us the SDC-specific identifiers for all relevant records. We save that to a table for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96e611-94c4-4618-8b94-90c1cfd70227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "org_meta = []\n",
    "for o in emma_orgs:\n",
    "    o_md = get_org_md(o)\n",
    "    for item in o_md:\n",
    "        org_meta.append({\n",
    "            \"org\": o,\n",
    "            \"md_id\": item[0],\n",
    "            \"md_pid\": item[1]\n",
    "        })\n",
    "pd.DataFrame(org_meta).to_parquet(\"data/emma_meta_inventory.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efa5a6-1bad-4912-99ec-4774ede7051e",
   "metadata": {},
   "source": [
    "I know, working with dataframes and Pandas is slow and there are better ways I need to learn. But it's convenient for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6647ba9-972f-4282-a6f0-2371d36222cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_meta = pd.read_parquet(\"data/emma_meta_inventory.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449a257-9683-4c39-89b2-a10fc5a2ed4c",
   "metadata": {},
   "source": [
    "I much prefer dealing with JSON/dictionaries over XML, so I add grab everything from the cache, transform, and add to the dataframe for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9973fed2-f8c7-4fb1-8ffc-309759bb1697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.13 s, sys: 437 ms, total: 9.57 s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_org_meta[\"meta\"] = df_org_meta.md_pid.apply(lambda x: meta_to_dict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5135c-941c-4ae8-8bc2-723829b653a5",
   "metadata": {},
   "source": [
    "I do a little bit of digestion of the metadata structures to tee up some information for examination. Right now, I'm interested in the lineage information and processing steps, in particular, so I summarize that a bit with some new properties in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc4d2f1b-f237-4c24-af9b-1507adca173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org_meta[\"meta_keys\"] = df_org_meta.meta.apply(lambda x: list(x[\"metadata\"].keys()) if \"metadata\" in x else None)\n",
    "df_org_meta[\"lineage\"] = df_org_meta.meta.apply(lambda x: x[\"metadata\"][\"dataqual\"][\"lineage\"] if \"metadata\" in x and \"dataqual\" in x[\"metadata\"] and \"lineage\" in x[\"metadata\"][\"dataqual\"] else None)\n",
    "df_org_meta[\"procstep\"] = df_org_meta.lineage.apply(lambda x: x[\"procstep\"] if x is not None and \"procstep\" in x else None)\n",
    "df_org_meta[\"num_procstep\"] = df_org_meta.procstep.apply(lambda x: len(x) if x is not None else 0)\n",
    "df_org_meta[\"procstep_s\"] = df_org_meta.procstep.apply(str)\n",
    "df_org_meta[\"dup_procstep\"] = df_org_meta.duplicated(subset=\"procstep_s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11415d8c-740a-42e5-a17d-7fb58c1b189e",
   "metadata": {},
   "source": [
    "At a really crude level, absolutely duplicative processing steps in metadata for what seem like they should be discrete data release products seems odd. Maybe these are really more of a serial data product that could be structured and handled in a different way in terms of review and other FSP process. Or maybe something else is going on. In the next few code blocks, I pull out this piece, figure out where I have have duplicate processing step metadata, show the highest number of those cases, and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d196758-16e9-4a8d-b621-57d13661811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_steps = df_org_meta[df_org_meta.num_procstep > 0][[\"md_pid\",\"procstep_s\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a98d084-894e-40d7-af0f-41e61fced44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_procsteps = proc_steps.groupby(\"procstep_s\").agg(list).reset_index(\"procstep_s\").rename(columns={\"md_pid\":\"procstep_pids\"})\n",
    "dup_procsteps[\"num_pids\"] = dup_procsteps.procstep_pids.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5405154f-e8e3-4616-9be7-e77b5532b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_procsteps.num_pids.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62099432-484e-471d-b48d-9f399e35bd91",
   "metadata": {},
   "source": [
    "This is an interesting example for further exploration. Upon examination of the PIDs where this duplication in processing steps occurred showed a somewhat different dynamic but still within the continuous data release problem area. These all appear to be aeromagnetic survey data collected through contracts, received by USGS in a particular format, and processed in a particular way for our own use. These particular metadata records originated with MRData, are likely generated dynamically from MRData, pre-dated the current data release process, and MRData is serving as the repository.\n",
    "\n",
    "We do need to dig a bit into what the current state is on processing geophysical survey data being collected now through things like EarthMRI contracts. What kind of review and approval process are these going through? Where are they being housed in terms of a repository (I believe the answer is ScienceBase to some extent)? Are we handling them in the most efficient way we can? Is a survey by survey organization of data products serving the community of use? Is there some larger data system these data could become part of? What's the relationship to other USGS assets where we are striving for National coverage like the National Map, and should we be looking toward more collaboration on processes and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4912523-b235-4f80-8769-8436458c0059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'procdesc': \"Conversion of measured values to geographic position and\\nmagnetic and radiometric values was performed by the contractor\\nusing industry standard practices.\\nDetails are found under Attribute Accuracy Report,\\nHorizontal_Position_Accuracy_Report, and\\nVertical_Position_Accuracy_Report\\nConversion processes, if reported, may be found in the\\nU.S. Department of Energy's published GJO- or GJBX- reports for the\\nquadrangle or group of quadrangles.  Unpublished products generated\\nby the contractor included magnetic tapes and perhaps some\\nwritten documentation.\",\n",
       "  'procdate': '1980'},\n",
       " {'procdesc': \"USGS reformatting of contractor data to standard format.\\nUSGS personnel used the software package Oasis Montaj version 6.3 by\\nGeosoft, Inc., to read in the original contractor's data.  Positioning\\nand magnetic values were checked for obvious errors or spikes. Values\\nof -9999.9,-999.9, -99.9, etc., were given where the value could not be\\nreasonably corrected or, in some cases, the whole record was removed.\\nInformation that was missing from the data file but recorded elsewhere, such\\nas year flown, was added th the file. The radiometric data were not checked\\nfor errors except for dummy values which were replaced with -99.9, -999.9, etc.\\nThe reformatted data files were written in the format described in the section\\non Entity_and_Attribute_Overview.\",\n",
       "  'procdate': '2008'},\n",
       " {'procdesc': 'Added keywords section with USGS persistent identifier as theme keyword.',\n",
       "  'procdate': '20201110',\n",
       "  'proccont': {'cntinfo': {'cntorgp': {'cntorg': 'U.S. Geological Survey',\n",
       "     'cntper': 'VeeAnn A. Cross'},\n",
       "    'cntpos': 'Marine Geologist',\n",
       "    'cntaddr': {'addrtype': 'Mailing and Physical',\n",
       "     'address': '384 Woods Hole Road',\n",
       "     'city': 'Woods Hole',\n",
       "     'state': 'MA',\n",
       "     'postal': '02543-1598'},\n",
       "    'cntvoice': '508-548-8700 x2251',\n",
       "    'cntfax': '508-457-2310',\n",
       "    'cntemail': 'vatnipp@usgs.gov'}}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(dup_procsteps[dup_procsteps.num_pids == 186].iloc[0].procstep_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af698d45-d6d2-4d34-a669-a424e6cac339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sdc]",
   "language": "python",
   "name": "conda-env-sdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
